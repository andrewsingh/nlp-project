#!/usr/bin/env python3

import spacy
import itertools
import numpy as np
import sys
from spacy import displacy
from spacy.symbols import ORTH
from collections import Counter
from collections import defaultdict
from rank_bm25 import BM25Okapi


YES_NO_WORDS = ["can", "could", "would", "is", "does", "has", "was", "were", "had", "have", "did", "are", "will"]
CHOICE_WORDS = ["or", "either"]
QUESTION_WORDS = ["who", "what", "where", "when", "why", "how", "whose", "which", "whom"]

ENTITY_MAP = defaultdict(lambda: [], 
              {"PERSON": ["PERSON", "ORG"],  # NER sometimes mistakes persons for orgs and vice versa
              "LOCATION": ["FAC", "GPE", "LOC"], # TODO: is this enough? Need to do some more testing
              "DATE": ["DATE", "TIME", "CARDINAL"], # Sometimes labels [year] BC as CARDINAL ORG
              "NUMBER": ["PERCENT", "MONEY", "QUANTITY", "CARDINAL", "ORDINAL"]})


# ============= simple helper functions =============

def get_text(path):
  def trim(lines):
    for i in range(len(lines)):
      line_txt = lines[i].replace("\n", "")
      if line_txt == "See also" or line_txt == "Notes" or line_txt == "References":
        return lines[:i]
    return lines

  with open(path, "r") as file:
    lines = file.readlines()
    sents = [line for line in trim(lines) if "." in line]

  return "".join(sents)


def get_questions(path):
    with open(path, "r") as file:
        return list(file.readlines())


def stem_and_lower(sent, no_stop=False):
  return [tok.lemma_.lower() for tok in sent if not tok.is_punct and not (no_stop and tok.is_stop)]


def process(sent, no_stop=False):
  return [tok for tok in sent if not tok.is_punct and not (no_stop and tok.is_stop)]


def intersection(a, b):
  return [elem for elem in a if elem in b]


def flatten(nested_list):
  return [elem for sub_list in nested_list for elem in sub_list]




# ============= more complex helper functions =============

def filter_sents_ner(doc, labels, query_ents):
  def has_label(sent):
    sent_labels = [ent.label_ for ent in sent.ents]
    for label in labels:
      if label in sent_labels:
        return True
    return False

  # backup in case an entity was mislabled
  def has_query_ent(sent):
    for ent in query_ents:
      """ as long as an ent in sent.ents has the same text as ent (label can be different), 
        'ent in sent.ents' will be True """
      if ent in sent.ents: 
        return True
    return False

  if len(labels) > 0:
    return [sent for sent in doc.sents if has_label(sent) or has_query_ent(sent)]
  else:
    return [sent for sent in doc.sents if len(stem_and_lower(sent)) > 0]


def process_question(question_text):
  question = nlp(question_text)
  question_word = ""
  question_start = -1

  has_yes_no = False
  answer_type = "NOCATEGORY"
  for (idx, word) in enumerate([tok.text.lower() for tok in question]):
    if word in QUESTION_WORDS:
      question_word = word
      question_start = idx
      break
    elif word in YES_NO_WORDS:
      has_yes_no = True
      if question_start < 0:
        question_start = idx
    elif word in CHOICE_WORDS:
      answer_type = "CHOICE"
      break
  
  if question_start > 0:
    question = question[question_start:] # trim question

  query_toks = process(question) # remove punctuation to get query tokens
  query_word = query_toks[0].text.lower()
  
  if len(question_word) > 0:
    query_toks = query_toks[1:] # remove question word
    if question_word in ["who", "whose", "whom"]:
        answer_type = "PERSON"
    elif question_word in ["what", "which"]:
      if query_word in ["time", "date", "hour", "day", "month", "year", "century"]:
        answer_type = "DATE"
      elif query_word in ["place", "location", "country", "state", "city", "town", "village"]:
        answer_type = "LOCATION"
      elif query_word in ["person", "man", "woman", "boy", "girl"]:
        answer_type = "PERSON"
      elif query_word in ["number", "percent", "percentage", "quantity", "amount", "price"]:
        answer_type = "NUMBER"
    elif question_word == "where":
        answer_type = "LOCATION"
    elif question_word == "when":
        answer_type = "DATE"
    elif question_word == "how":
        if query_word in ["few", "little", "much", "many"]:
            answer_type = "NUMBER"
            query_toks = query_toks[1:]
        elif query_word in ["young", "old", "long"]:
            answer_type = "DATE"
            query_toks = query_toks[1:]
  elif has_yes_no and answer_type != "CHOICE":
    answer_type = "YESNO"

  query = nlp(" ".join([tok.text for tok in query_toks]))
  return (answer_type, query)


def get_bm25(query, labels, n):
  query_stem_lower = stem_and_lower(query)
  sents_filtered = filter_sents_ner(doc, labels, query.ents)
  sents_processed = [stem_and_lower(sent) for sent in sents_filtered]
  bm25 = BM25Okapi(sents_processed)
  scored_sents = list(zip(bm25.get_scores(query_stem_lower), sents_filtered))
  scored_sents.sort(key=lambda x: x[0], reverse=True)
  scored_sents = [(round(entry[0], 1), entry[1]) for entry in scored_sents]
  return scored_sents[:n]


def pattern_match(question_text):
  def match_defn12(sent, subj, defn_num):
    subj_word = subj.text.lower()
    if defn_num == 1:
      subj_dep = "nsubj"
    else:
      subj_dep = "attr"
    for tok in sent:
      if tok.lemma_ == "be" and tok.pos_ in ["VERB", "AUX"]:
        tok_children_text = [child.text.lower() for child in list(tok.children)]
        if subj_word in tok_children_text:
          subj_idx = tok_children_text.index(subj_word)
          if list(tok.children)[subj_idx].dep_ == subj_dep:
            return True
    return False
    
  def match_defn_appos(sent, subj):
    subj_word = subj.text.lower()
    for tok in sent:
      if tok.text.lower() == subj_word and (tok.pos_ in ["NOUN", "PROPN"]) \
        and "appos" in [child.dep_ for child in tok.children]:
          return True
    return False

  question = nlp(question_text)
  question_processed = process(question)
  question_stem_lower = stem_and_lower(question)
  if question_stem_lower[0] in ["who", "what"] and question_stem_lower[1] == "be":
    subjs = [tok for tok in list(question_processed[1].children) if (tok.dep_ in ["attr", "nsubj"]) \
              and (tok.pos_ in ["NOUN", "PROPN"])]
    if len(subjs) == 1:
      subj = subjs[0]
      matches1 = [sent for sent in doc.sents if match_defn12(sent, subj, 1)]
      matches2 = [sent for sent in doc.sents if match_defn12(sent, subj, 2)]
      matches_appos = [sent for sent in doc.sents if match_defn_appos(sent, subj)]
      return [matches1, matches2, matches_appos]
    
  return []



# ============= main question-answering function =============

def get_best_sent(question_text):
  (answer_type, query) = process_question(question_text)
  pattern_matches = flatten(pattern_match(question_text))
  if len(pattern_matches) > 0:
    return pattern_matches[0].text
  else:
    labels = ENTITY_MAP[answer_type]
    ranked_sents = get_bm25(query, labels, 1)
    if len(ranked_sents) > 0:
      return ranked_sents[0][1].text
    else:
      return list(doc.sents)[0].text

  

if __name__ == '__main__':
  args = sys.argv
  if len(args) >= 3:
    article_path = str(args[1])
    questions_path = str(args[2])
    text = get_text(article_path)
    questions = get_questions(questions_path)
    nlp = spacy.load("en_core_web_lg")
    sc1 = [{ORTH: "ca."}]
    nlp.tokenizer.add_special_case("ca.", sc1)
    doc = nlp(text)
    for question in questions:
        print(get_best_sent(question).replace("\n", ""))

    



