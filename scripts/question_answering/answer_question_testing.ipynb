{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "from spacy import displacy\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "sc1 = [{ORTH: \"ca.\"}]\n",
    "nlp.tokenizer.add_special_case(\"ca.\", sc1)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "bert_model = BertForQuestionAnswering.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_NO_WORDS = [\"am\", \"can\", \"could\", \"would\", \"is\", \"does\", \"has\", \"was\", \"were\", \"had\", \\\n",
    "                \"have\", \"did\", \"are\", \"will\", \"do\", \"may\", \"might\", \"shall\", \"should\", \\\n",
    "                 \"must\", \"ought\", \"couldn't\", \"shouldn't\", \"wouldn't\", \"hasn't\", \"weren't\"]\n",
    "\n",
    "YES_NO_STEMS = [\"may\", \"be\", \"would\", \"should\", \"do\", \"ought\", \"will\", \"must\", \"might\", \"have\", \"shall\", \"could\", \"can\"]\n",
    "CHOICE_WORDS = [\"or\", \"either\"]\n",
    "QUESTION_WORDS = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\"]\n",
    "\n",
    "FIRST_PRONS = [\"I\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\", \"myself\", \"ourselves\"]\n",
    "SECOND_PRONS = [\"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "\n",
    "ENTITY_MAP = defaultdict(lambda: [], \n",
    "              {\"PERSON\": [\"PERSON\", \"ORG\"],  # NER sometimes mistakes persons for orgs and vice versa\n",
    "              \"LOCATION\": [\"FAC\", \"GPE\", \"LOC\", \"NORP\", \"ORG\"], # TODO: is ORG really necessary here?\n",
    "              \"DATE\": [\"DATE\", \"TIME\", \"CARDINAL\"], # Sometimes labels [year] BC as CARDINAL ORG\n",
    "              \"NUMBER\": [\"PERCENT\", \"MONEY\", \"QUANTITY\", \"CARDINAL\", \"ORDINAL\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= simple helper functions =============\n",
    "\n",
    "def get_text(path):\n",
    "  def trim(lines):\n",
    "    for i in range(len(lines)):\n",
    "      line_txt = lines[i].replace(\"\\n\", \"\")\n",
    "      if line_txt == \"See also\" or line_txt == \"Notes\" or line_txt == \"References\":\n",
    "        return lines[:i]\n",
    "    return lines\n",
    "\n",
    "  with open(path, \"r\") as file:\n",
    "    #text = file.read()\n",
    "    lines = file.readlines()\n",
    "    sents = [line for line in trim(lines) if \".\" in line]\n",
    "\n",
    "  return \"\".join(sents)\n",
    "\n",
    "\n",
    "def get_questions(path):\n",
    "  with open(path, \"r\") as file:\n",
    "    return [line.replace(\"\\n\", \"\").strip() for line in file.readlines()]    \n",
    "\n",
    "  \n",
    "def get_test_cases(path):\n",
    "  with open(path, \"r\") as file:\n",
    "    return [line.replace(\"\\n\", \"\").strip().split(\"[SEP]\") for line in file.readlines()]  \n",
    "\n",
    "\n",
    "def print_list(l):\n",
    "  for elem in l:\n",
    "    print(elem)\n",
    "\n",
    "\n",
    "def print_list2(l):\n",
    "  for l2 in l:\n",
    "    for elem in l2:\n",
    "      print(elem)\n",
    "\n",
    "\n",
    "def serve_dep(ex):\n",
    "  displacy.serve(ex, style=\"dep\")\n",
    "\n",
    "\n",
    "def stem_and_lower(sent, no_stop=False):\n",
    "  return [tok.lemma_.lower() for tok in sent if not tok.is_punct and not (no_stop and tok.is_stop)]\n",
    "\n",
    "\n",
    "def process(sent, no_stop=False):\n",
    "  return [tok for tok in sent if not tok.is_punct and not (no_stop and tok.is_stop)]\n",
    "\n",
    "\n",
    "def intersection(a, b):\n",
    "  return [elem for elem in a if elem in b]\n",
    "\n",
    "\n",
    "def flatten(nested_list):\n",
    "  return [elem for sub_list in nested_list for elem in sub_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= more complex helper functions =============\n",
    "\n",
    "def get_question_ents(question):\n",
    "  question_text = question.text\n",
    "  ents_from_text = [((ent.text, ent.label_), ent) for ent in doc.ents if ent.text.lower() in question_text.lower()]\n",
    "  ents_from_question = [((ent.text, ent.label_), ent) for ent in question.ents]\n",
    "  return list(dict(ents_from_text + ents_from_question).values())\n",
    "\n",
    "\n",
    "def stem_query(query, sent):\n",
    "  result = []\n",
    "  query_processed = process(query)\n",
    "  sent_processed = process(sent)\n",
    "  sent_toks = list(sent_processed)\n",
    "  sent_toks_text = [tok.text for tok in sent_toks]\n",
    "  for tok in query_processed:\n",
    "    if tok.text in sent_toks_text:\n",
    "      idx = sent_toks_text.index(tok.text)\n",
    "      result.append(sent_toks[idx].lemma_.lower())\n",
    "    else:\n",
    "      result.append(tok.lemma_.lower())\n",
    "  return result\n",
    "  \n",
    "\n",
    "def filter_sents_ner(qword, qents, labels):\n",
    "  \n",
    "  def has_label(sent, exclude_ents_text):\n",
    "    sent_labels = [ent.label_ for ent in sent.ents if ent.text not in exclude_ents_text]\n",
    "    for label in labels:\n",
    "      if label in sent_labels:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "  # backup in case an entity was mislabled\n",
    "  def has_query_ent(sent):\n",
    "    for ent in qents:\n",
    "      if ent.text in [sent_ent.text for sent_ent in sent.ents]: \n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "  if len(labels) > 0:\n",
    "    exclude_ents_text = []\n",
    "    if qword == \"where\":\n",
    "      exclude_ents_text = list(set([ent.text for ent in qents if ent.label_ in ENTITY_MAP[\"LOCATION\"]]))\n",
    "      #print(\"exclude_ents_text: {}\".format(exclude_ents_text))\n",
    "    return [sent for sent in doc.sents if has_label(sent, exclude_ents_text)]\n",
    "  else:\n",
    "    return [sent for sent in doc.sents if len(stem_and_lower(sent)) > 0]\n",
    "\n",
    "\n",
    "def process_question(question_text):\n",
    "  question = nlp(question_text)\n",
    "  question_word = \"\"\n",
    "  question_start = -1\n",
    "  last_pron_idx = -1\n",
    "\n",
    "  has_yes_no = False\n",
    "  answer_type = \"NOCATEGORY\"\n",
    "  \n",
    "  for (idx, tok) in enumerate([tok for tok in question]):\n",
    "    if tok.text.lower() in QUESTION_WORDS:\n",
    "      question_word = tok.text.lower()\n",
    "      #question_start = idx\n",
    "      break\n",
    "    elif tok.lemma_.lower() in YES_NO_STEMS:\n",
    "      has_yes_no = True\n",
    "    elif tok.text.lower() in CHOICE_WORDS:\n",
    "      answer_type = \"CHOICE\"\n",
    "    \n",
    "    if tok.text.lower() in (FIRST_PRONS + SECOND_PRONS):\n",
    "      last_pron_idx = idx\n",
    "\n",
    "      \n",
    "  if last_pron_idx >= 0 and last_pron_idx < len(question) - 1:\n",
    "    question = question[last_pron_idx + 1:]\n",
    "\n",
    "  query_toks = process(question) # remove punctuation to get query tokens\n",
    "  query_word = query_toks[0].text.lower()\n",
    "  \n",
    "  if len(question_word) > 0:\n",
    "    if query_word == question_word:\n",
    "      query_toks = query_toks[1:] # remove question word\n",
    "    if question_word in [\"who\", \"whose\", \"whom\"]:\n",
    "        answer_type = \"PERSON\"\n",
    "    elif question_word in [\"what\", \"which\"]:\n",
    "      if query_word in [\"time\", \"date\", \"hour\", \"day\", \"month\", \"year\", \"century\"]:\n",
    "        answer_type = \"DATE\"\n",
    "      elif query_word in [\"place\", \"location\", \"country\", \"state\", \"city\", \"town\", \"village\"]:\n",
    "        answer_type = \"LOCATION\"\n",
    "      elif query_word in [\"person\", \"man\", \"woman\", \"boy\", \"girl\"]:\n",
    "        answer_type = \"PERSON\"\n",
    "      elif query_word in [\"number\", \"percent\", \"percentage\", \"quantity\", \"amount\", \"price\"]:\n",
    "        answer_type = \"NUMBER\"\n",
    "    elif question_word == \"where\":\n",
    "        answer_type = \"LOCATION\"\n",
    "    elif question_word == \"when\":\n",
    "        answer_type = \"DATE\"\n",
    "    elif question_word == \"how\":\n",
    "        if query_word in [\"few\", \"little\", \"much\", \"many\"]:\n",
    "            answer_type = \"NUMBER\"\n",
    "            query_toks = query_toks[1:]\n",
    "        elif query_word in [\"young\", \"old\", \"long\"]:\n",
    "            answer_type = \"DATE\"\n",
    "            query_toks = query_toks[1:]\n",
    "  elif has_yes_no and answer_type != \"CHOICE\":\n",
    "    answer_type = \"YESNO\"\n",
    "\n",
    "  query = nlp(\" \".join([tok.text for tok in query_toks]))\n",
    "  return (answer_type, query, question)\n",
    "\n",
    "\n",
    "def lccs(query_tokenized, sent_tokenized):\n",
    "  mat = np.zeros((len(query_tokenized) + 1, len(sent_tokenized) + 1))\n",
    "  for i in range(1, len(query_tokenized) + 1):\n",
    "    for j in range(1, len(sent_tokenized) + 1):\n",
    "      if query_tokenized[i - 1] == sent_tokenized[j - 1]:\n",
    "        mat[i][j] = mat[i-1][j-1] + 1\n",
    "  return np.max(mat)\n",
    "\n",
    "\n",
    "def ave_dist_sent(query, sent_stem_lower, no_stop=False):\n",
    "  def get_dist(sent_stem_lower, a, b):\n",
    "    if a in sent_stem_lower and b in sent_stem_lower:\n",
    "      last_a = -1\n",
    "      last_b = -1\n",
    "      pairs = []\n",
    "      for i in range(len(sent_stem_lower)):\n",
    "        if sent_stem_lower[i] == a:\n",
    "          last_a = i\n",
    "          if last_b >= 0:\n",
    "            pairs.append((last_a, last_b))\n",
    "        elif sent_stem_lower[i] == b:\n",
    "          last_b = i\n",
    "          if last_a >= 0:\n",
    "            pairs.append((last_b, last_a))\n",
    "      return min([abs(pair[0] - pair[1]) for pair in pairs])\n",
    "    else:\n",
    "      return len(sent_stem_lower)\n",
    "\n",
    "  query_stem_lower = stem_and_lower(query, no_stop)\n",
    "  query_unique_toks = list(set(query_stem_lower))\n",
    "  pair_indices = list(itertools.combinations(range(len(query_unique_toks)), 2))\n",
    "  return sum([get_dist(sent_stem_lower, query_unique_toks[pair[0]], query_unique_toks[pair[1]]) for pair in pair_indices]) / len(pair_indices)\n",
    "\n",
    "\n",
    "def get_bm25(query, sents_filtered, n):\n",
    "  queries_stem = [stem_query(query, sent) for sent in sents_filtered]\n",
    "  sents_stem_lower = [stem_and_lower(sent) for sent in sents_filtered]\n",
    "  bm25 = BM25Okapi(sents_stem_lower)\n",
    "  assert(len(queries_stem) == len(sents_filtered))\n",
    "  scored_sents = [(bm25.get_scores(queries_stem[i])[i], sents_filtered[i]) for i in range(len(sents_filtered))]\n",
    "  assert(len(scored_sents) == len(sents_filtered))\n",
    "  #ave_dist_scores = [ave_dist_sent(query, sent_stem_lower) for sent_stem_lower in sents_stem_lower]\n",
    "  #lccs_scores = [lccs(query_stem_lower, sent_stem_lower) for sent_stem_lower in sents_stem_lower]\n",
    "  #print(ave_dist_scores)\n",
    "  #scored_sents = list(zip(bm25.get_scores(query_stem_lower), ave_dist_scores, lccs_scores, sents_filtered))\n",
    "  scored_sents.sort(key=lambda x: x[0], reverse=True)\n",
    "  #scored_sents = [(round(entry[0], 1), round(entry[1], 1), entry[2], entry[3]) for entry in scored_sents]\n",
    "  scored_sents = [(round(entry[0], 1), entry[1]) for entry in scored_sents]\n",
    "  return scored_sents[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add comments\n",
    "def pattern_match(question, question_ents, sents_filtered):\n",
    "  \n",
    "  question_processed = process(question)\n",
    "  question_stem_lower = stem_and_lower(question)\n",
    "  qword = question_stem_lower[0]\n",
    "  \n",
    "  def match_defn12(sent, subj, defn_num, subj_phrase_lower=None):\n",
    "    # match the first two patterns (x is answer, answer is x)\n",
    "    # if defn_num is 1, matches first pattern, else matches second pattern\n",
    "    subj_word = subj.text.lower()\n",
    "    if defn_num == 1:\n",
    "      subj_dep = \"nsubj\"\n",
    "    else:\n",
    "      subj_dep = \"attr\"\n",
    "    for tok in sent:\n",
    "      if tok.lemma_ == \"be\" and tok.pos_ in [\"VERB\", \"AUX\"]: # is, are, am, be, etc.\n",
    "        # get the children of the aux verb in the dependency parse\n",
    "        tok_children_text = [child.text.lower() for child in list(tok.children)]\n",
    "        if subj_word in tok_children_text:\n",
    "          subj_idx = tok_children_text.index(subj_word)\n",
    "          subj_in_sent = list(tok.children)[subj_idx]\n",
    "          if subj_in_sent.dep_ == subj_dep:\n",
    "            if subj_phrase_lower == None:\n",
    "              return True\n",
    "            else:\n",
    "              # Handle multi-word subjects (subj_phrase_lower is not None)\n",
    "              subj_phrase_lower_in_sent = \" \".join(list([tok2.text.lower() for tok2 in subj_in_sent.lefts]) + [subj_in_sent.text.lower()])\n",
    "              return subj_phrase_lower in subj_phrase_lower_in_sent\n",
    "    return False\n",
    "    \n",
    "    \n",
    "  def match_defn_appos(sent, subj, subj_phrase_lower=None):\n",
    "    subj_word = subj.text.lower()\n",
    "    for tok in sent:\n",
    "      if tok.text.lower() == subj_word and (tok.pos_ in [\"NOUN\", \"PROPN\"]) \\\n",
    "        and (tok.dep_ == \"appos\" or \"appos\" in [child.dep_ for child in tok.children]):\n",
    "        if subj_phrase_lower == None:\n",
    "          return True\n",
    "        else:\n",
    "          # Handle multi-word subjects (subj_phrase_lower is not None)\n",
    "          subj_phrase_lower_in_sent = \" \".join(list([tok2.text.lower() for tok2 in tok.lefts]) + [tok.text.lower()])\n",
    "          return subj_phrase_lower in subj_phrase_lower_in_sent\n",
    "    return False\n",
    "\n",
    "  \n",
    "  def get_matches(qword, subj, subj_phrase_lower=None):\n",
    "    matches1 = [sent for sent in sents_filtered if match_defn12(sent, subj, 1, subj_phrase_lower)]\n",
    "    matches2 = [sent for sent in sents_filtered if match_defn12(sent, subj, 2, subj_phrase_lower)]\n",
    "    matches_appos = [sent for sent in sents_filtered if match_defn_appos(sent, subj, subj_phrase_lower)]\n",
    "    return [matches1, matches2, matches_appos]\n",
    "  \n",
    "\n",
    "  if qword in [\"who\", \"what\", \"where\", \"when\"]:\n",
    "    #  and question_stem_lower[1] == \"be\"\n",
    "    question_verbs = [tok for tok in question_processed if tok.pos_ in [\"VERB\", \"AUX\"]]\n",
    "    \"\"\" question should only have one verb; we want to pattern match \"What is a dog\", but not\n",
    "    \"What is a dog classified as\" (the ranking function can handle the second example) \n",
    "    \"\"\"\n",
    "    \n",
    "#     if verbose:\n",
    "#       print(\"Question verbs: {}\".format(question_verbs))\n",
    "      \n",
    "    # Special case where we allow \"where\" questions to have a second verb: \"located\"\n",
    "    # (for example, \"Where is New York located?\")\n",
    "    where_located_case = (question_stem_lower[0] == \"where\" and len(question_verbs) == 2 and question_verbs[1].lemma_.lower() == \"locate\")\n",
    "    if (len(question_verbs) == 1 and question_verbs[0].lemma_.lower() == \"be\") or where_located_case:\n",
    "      verb_idx = 0\n",
    "      if where_located_case:\n",
    "        verb_idx = 1\n",
    "      \n",
    "#       if verbose:\n",
    "#         print(\"Subj options: {}\".format([tok for tok in list(question_verbs[verb_idx].children)]))\n",
    "        \n",
    "      subjs = [tok for tok in list(question_verbs[verb_idx].children) if (tok.dep_ in [\"attr\", \"nsubj\"]) \\\n",
    "        and (tok.pos_ in [\"NOUN\", \"PROPN\"])]\n",
    "      if verbose:\n",
    "        #print(\"subj options: {}\".format(list(question_verbs[verb_idx].children)))\n",
    "        print(\"subjs: \" + str(subjs))\n",
    "      if len(subjs) == 1:\n",
    "        subj = subjs[0]\n",
    "        # Make sure the subject is an entity, not a common noun\n",
    "        # (although should try to handle this case as well)\n",
    "        #if len(nlp(\"{} is {}?\".format(question_processed[0].text, subj)).ents) >= 1:\n",
    "        if len(question_ents) >= 1:\n",
    "          if qword == \"who\":\n",
    "            if len(get_question_ents(nlp(\"Who is {}\".format(subj)))) >= 1:\n",
    "              # \"who\" question, can just use the single-word subj (likely a name)\n",
    "              # TODO: what if subj is a last name, and multiple ents in the article share that last name?\n",
    "              if verbose:\n",
    "                print(\"Pattern match subj: \" + subj.text)\n",
    "              return get_matches(qword, subj)\n",
    "          else:\n",
    "            # not a \"who\" question, get the whole entity\n",
    "            subj_phrase_lower = min([ent.text.lower() for ent in question_ents], key=lambda x: len(x))\n",
    "            if verbose:\n",
    "              print(\"subj_phrase_lower: {}\".format(subj_phrase_lower))\n",
    "            if subj.text.lower() in subj_phrase_lower:\n",
    "              if subj.text.lower() == subj_phrase_lower:\n",
    "                return get_matches(qword, subj)\n",
    "              if verbose:\n",
    "                print(\"Pattern match subj phrase: \" + subj_phrase_lower)\n",
    "              return get_matches(qword, subj, subj_phrase_lower)\n",
    "              \n",
    "  return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(question_text, sentence_text):\n",
    "  if verbose:\n",
    "    #print(\"Question:\\n{}\".format(question_text))\n",
    "    print(\"Sentence:\\n{}\".format(sentence_text))\n",
    "\n",
    "  question_ids = bert_tokenizer.encode(question_text)\n",
    "  sentence_ids = bert_tokenizer.encode(sentence_text)\n",
    "\n",
    "  input_ids = bert_tokenizer.build_inputs_with_special_tokens(question_ids, sentence_ids)\n",
    "  input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#   if verbose:\n",
    "#     print(\"Tokens:\\n{}\".format(input_tokens))\n",
    "  token_type_ids = bert_tokenizer.create_token_type_ids_from_sequences(question_ids, sentence_ids)\n",
    "  start_logits, end_logits = bert_model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "  \n",
    "  answer_score = float(torch.max(start_logits)) + float(torch.max(end_logits))\n",
    "  \n",
    "  #print(\"Confidence: {}\".format(answer_score))\n",
    "  \n",
    "  answer_preprocessed = \" \".join(input_tokens[torch.argmax(start_logits) : torch.argmax(end_logits) + 1]).replace(\" ##\", \"\")\n",
    "  sub1 = re.sub(r\"(.) ' (.{1,2})\\b\", r\"\\1'\\2\", answer_preprocessed)\n",
    "  sub2 = re.sub(r\"(.) ' (.)\", r\"\\1' \\2\", sub1)\n",
    "  sub3 = re.sub(r\"(.) , (.)\", r\"\\1, \\2\", sub2)\n",
    "  sub4 = re.sub(r\" \\)\", r\")\", sub3)\n",
    "  sub5 = re.sub(r\"\\( \", r\"(\", sub4)\n",
    "  return sub5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_bert(question_text, sentence_text):\n",
    "  question_ids = bert_tokenizer.encode(question_text)\n",
    "  sentence_ids = bert_tokenizer.encode(sentence_text)\n",
    "\n",
    "  input_ids = bert_tokenizer.build_inputs_with_special_tokens(question_ids, sentence_ids)\n",
    "  input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "  token_type_ids = bert_tokenizer.create_token_type_ids_from_sequences(question_ids, sentence_ids)\n",
    "  start_logits, end_logits = bert_model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "  \n",
    "  answer_preprocessed = \" \".join(input_tokens[torch.argmax(start_logits) : torch.argmax(end_logits) + 1]).replace(\" ##\", \"\")\n",
    "  sub1 = re.sub(r\"(.) ' (.{1,2})\\b\", r\"\\1'\\2\", answer_preprocessed)\n",
    "  sub2 = re.sub(r\"(.) ' (.)\", r\"\\1' \\2\", sub1)\n",
    "  sub3 = re.sub(r\"(.) , (.)\", r\"\\1, \\2\", sub2)\n",
    "  sub4 = re.sub(r\" \\)\", r\")\", sub3)\n",
    "  sub5 = re.sub(r\"\\( \", r\"(\", sub4)\n",
    "\n",
    "  return (sub5, (float(torch.max(start_logits)) + float(torch.max(end_logits))))\n",
    "  \n",
    "  \n",
    "def answer_question_bert(question_text):\n",
    "  entries = [get_answer_bert(question_text, sent.text) for sent in doc.sents]\n",
    "  #entries = [get_answer_bert(question_text, passage.replace(\"\\n\", \"\").strip()) for passage in passages]\n",
    "  return max(entries, key=lambda x: x[1])[0]\n",
    "\n",
    "  \n",
    "def rank_sents_bert(question_text, n):\n",
    "  entries = [(get_answer_bert(question_text, sent.text)[1], sent) for sent in doc.sents]\n",
    "  entries.sort(key=lambda x: x[0], reverse=True)\n",
    "  return [(round(x[0], 2), x[1]) for x in entries][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extract_answer(idx, sent=None):\n",
    "  if sent == None:\n",
    "    sent = get_best_sent(sample_questions[idx])\n",
    "  return extract_answer(sample_questions[idx], sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_yes_no(question_triple, question_ents):\n",
    "  \n",
    "  (answer_type, query, question) = question_triple\n",
    "  subj = next((tok for tok in question if tok.dep_ == \"nsubj\"), None)\n",
    "  \n",
    "  if verbose:\n",
    "    print(\"subj: {}\".format(subj))\n",
    "\n",
    "  if subj != None:\n",
    "    subj_phrase_lower = subj.text.lower()\n",
    "    subj_ent = min([ent for ent in question_ents if subj.text.lower() in ent.text.lower()], key=lambda ent: len(ent.text))\n",
    "    \n",
    "    if subj_ent != None and subj_ent.label_ != \"PERSON\":\n",
    "      subj_phrase_lower = subj_ent.text.lower()\n",
    "    \n",
    "    if verbose:\n",
    "      print(\"subj phrase lower: {}\".format(subj_phrase_lower))\n",
    "      \n",
    "    ranked_sents = get_bm25(query, list(doc.sents), 5)\n",
    "    best_sent = ranked_sents[0][1].text.replace(\"\\n\", \"\")\n",
    "    best_sent_score = ranked_sents[0][0]\n",
    "    if verbose:\n",
    "      print(\"best sent: {}\".format(best_sent))\n",
    "      print(\"best sent score: {}\".format(best_sent_score))\n",
    "      \n",
    "    pred = question[subj.i+1:]\n",
    "    match_words = stem_and_lower(pred, no_stop=True)\n",
    "    \n",
    "    if verbose:\n",
    "      print(\"match_words: {}\".format(match_words))\n",
    "      \n",
    "    for (score, sent) in ranked_sents:\n",
    "      if subj_phrase_lower in sent.text.lower():\n",
    "        sent_words = stem_and_lower(sent)\n",
    "        if set(intersection(match_words, sent_words)) == set(match_words):\n",
    "          return True\n",
    "    \n",
    "  return False\n",
    "    \n",
    "    \n",
    "#     if best_sent_score < 10 and (list(subj.ancestors)[0].lemma_ == \"be\" and subj.i < len(question) - 1) and False:\n",
    "#       if verbose:\n",
    "#         print(\"Approach: pattern matching\")\n",
    "#       # above array access is safe since subj is guaranteed to have an ancestor\n",
    "#       pred = question[subj.i+1:]\n",
    "#       match_words = stem_and_lower(pred, no_stop=True)\n",
    "      \n",
    "#       if verbose:\n",
    "#         print(\"match_words: {}\".format(match_words))\n",
    "    \n",
    "#       if subj_ent != None and subj_ent.label_ == \"PERSON\":\n",
    "#         qstart = \"Who\"\n",
    "#       else:\n",
    "#         qstart = \"What\"\n",
    "#       question_pm = nlp(\"{} is {}\".format(qstart, subj_ent.text))\n",
    "#       pattern_matches_flat = flatten(pattern_match(question_pm, get_question_ents(question_pm), \\\n",
    "#         [sent for sent in doc.sents if subj_phrase_lower in sent.text]))\n",
    "#       pattern_matches_words = [stem_and_lower(pm) for pm in pattern_matches_flat]\n",
    "      \n",
    "#       if verbose:\n",
    "#         print(\"pattern_matches_words: {}\".format(pattern_matches_words))\n",
    "        \n",
    "#       for pmw in pattern_matches_words:\n",
    "#         if set(intersection(match_words, pmw)) == set(match_words):\n",
    "#           return True\n",
    "#       return False\n",
    "#     else:\n",
    "#       if verbose:\n",
    "#         print(\"Approach: ranking\")\n",
    "        \n",
    "      \n",
    "      \n",
    "#       return subj_phrase_lower in best_sent.lower()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_sent(question_triple, question_ents, test=False):\n",
    "  (answer_type, query, question) = question_triple\n",
    "  question_stem_lower = stem_and_lower(question)\n",
    "  #question_ents = get_question_ents(question)\n",
    "  qword = question_stem_lower[0]\n",
    "    \n",
    "  sents_filtered = list(doc.sents)\n",
    "  if qword == \"where\":\n",
    "    sents_filtered = filter_sents_ner(qword, question_ents, ENTITY_MAP[\"LOCATION\"])\n",
    "  elif qword == \"when\":\n",
    "    sents_filtered = filter_sents_ner(qword, question_ents, ENTITY_MAP[\"DATE\"])\n",
    "    \n",
    "  pattern_matches = pattern_match(question, question_ents, sents_filtered)\n",
    "  pattern_matches_flat = flatten(pattern_matches)\n",
    "  if len(pattern_matches_flat) > 0:\n",
    "    if verbose:\n",
    "      print(\"Retrieval method: pattern match\")\n",
    "    if test:\n",
    "      return pattern_matches\n",
    "    else:\n",
    "      return pattern_matches_flat[0].text.replace(\"\\n\", \"\")\n",
    "  else:\n",
    "    if verbose:\n",
    "      print(\"Retrieval method: ranking function\")\n",
    "    \n",
    "    sents_filtered = filter_sents_ner(qword, question_ents, ENTITY_MAP[answer_type])\n",
    "    if verbose:\n",
    "      print(\"Sents remaining: {} of {}\".format(len(sents_filtered), len(list(doc.sents))))\n",
    "    if len(sents_filtered) == 0:\n",
    "      sents_filtered = list(doc.sents)\n",
    "      if verbose or test:\n",
    "        print(\"Warning: all sentences filtered out, skipping NER filter step\")\n",
    "    \n",
    "    ranked_sents = get_bm25(query, sents_filtered, 5)\n",
    "    if len(ranked_sents) > 0:\n",
    "      if test:\n",
    "        return ranked_sents\n",
    "      else:\n",
    "        return ranked_sents[0][1].text.replace(\"\\n\", \"\")\n",
    "    else:\n",
    "      return \"FAIL\"\n",
    "      #return list(doc.sents)[0].text.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question_text):\n",
    "  (answer_type, query, question) = process_question(question_text)\n",
    "  question_ents = get_question_ents(question)\n",
    "  \n",
    "  if verbose:\n",
    "    print(\"Question: {}\\nQuery: {}\\nAnswer type: {}\".format(question.text, query, answer_type))\n",
    "    \n",
    "  if answer_type == \"YESNO\":\n",
    "    ans_bool = answer_yes_no((answer_type, query, question), question_ents)\n",
    "    if ans_bool:\n",
    "      return \"yes\"\n",
    "    else:\n",
    "      return \"no\"\n",
    "  else:\n",
    "    best_sent = get_best_sent((answer_type, query, question), question_ents)\n",
    "    return extract_answer(question.text, best_sent)\n",
    "\n",
    "\n",
    "def run_tests(bert=False):\n",
    "  test_cases = get_test_cases(\"../../tests/s{}a{}test.txt\".format(set_num, doc_num))\n",
    "  all_tests_passed = True\n",
    "  for (question_text, answer_text) in test_cases:\n",
    "    if bert:\n",
    "      system_answer = answer_question_bert(question_text)\n",
    "    else:\n",
    "      system_answer = answer_question(question_text)\n",
    "    if system_answer == answer_text:\n",
    "      print(\"Test passed!\")\n",
    "    else:\n",
    "      all_tests_passed = False\n",
    "      print(\"Test failed\\nQuestion: {}\\nAnswer: {}\\nYour answer: {}\".format(question_text, answer_text, system_answer))\n",
    "  if all_tests_passed:\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "\n",
    "def update_tests():\n",
    "  test_cases = get_test_cases(\"../../tests/s{}a{}.txt\".format(set_num, doc_num))\n",
    "  with open(path, \"w+\") as file:\n",
    "    for (question_text, _) in test_cases:\n",
    "      system_answer = answer_question(question_text)\n",
    "      file.write(question_text + \"[SEP]\" + system_answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_num = 1\n",
    "doc_num = 1\n",
    "text = get_text(\"../../data/Development_data/set{}/a{}.txt\".format(set_num, doc_num))\n",
    "passages = text.split(\"\\n\") \n",
    "doc = nlp(text)\n",
    "sample_questions = get_questions(\"../../questions/s{}a{}.txt\".format(set_num, doc_num))\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_tests()\n",
    "#run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../../tests/s{}a{}.txt\".format(set_num, doc_num)\n",
    "# test_questions = get_questions(path)\n",
    "# print(test_questions)\n",
    "# with open(path, \"w+\") as file:\n",
    "#   for question_text in test_questions:\n",
    "#     answer = extract_answer(question_text, get_best_sent(question_text))\n",
    "#     file.write(question_text + \"[SEP]\" + answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = sample_questions[15]\n",
    "# print(q)\n",
    "#q = \"What is the nickname of dogs?\"\n",
    "#get_best_sent(\"Who came after Sneferu?\", test=True)\n",
    "# for q in sample_questions:\n",
    "#   print(\"Question: {}\".format(q))\n",
    "#   print(\"Base Answer: {}\".format(answer_question(q)))\n",
    "#   print(\"BERT Answer: {}\".format(answer_question_bert(q)))\n",
    "#print_list(rank_sents_bert(q, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \"What is countershading?\"\n",
    "# qp = process(nlp(q))\n",
    "# print(list(qp[1].children))\n",
    "# displacy.render(nlp(q), style=\"dep\", jupyter=True)\n",
    "\n",
    "#extract_answer(\"Who is Sahure?\", \"Userkaf was succeeded by Userkaf's son Sahure (2487–2475 BC) who commanded an expedition to Punt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# qs = [\"Please tell me, Sahure commanded an expedition to what city?\",\n",
    "#        \"Who is Sahure?\",\n",
    "#        \"Can you tell me who Sahure is?\",\n",
    "#        \"Would you mind telling me who Sahure is?\",\n",
    "#        \"Please tell me was the first Pharaoh Djoser or Sahure?\",\n",
    "#        #\"Please tell me if Sahure is a king?\",\n",
    "#        \"Might I ask of you, who is Sahure?\",\n",
    "#        \"Djoser was the first Pharaoh of what kingdom?\",\n",
    "#        \"In honor of whom was the Sphinx built?\",\n",
    "#         \"By whom was the Sphinx built?\"\n",
    "#        #\"Might you tell me was Sahure a king?\"\n",
    "#      ]\n",
    "\n",
    "# for q in qs:\n",
    "#   print(q)\n",
    "#   print(process_question(q))\n",
    "#   print(answer_question(q))\n",
    "#   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#q = \"Mustn't Menkauhor Kaiu be one of the last Pharaohs of the Fifth Dynasty?\"\n",
    "#q = \"Was Menkauhor Kaiu one of the last Pharaohs of the Fifth Dynasty?\"\n",
    "q = \"Was Djoser the first pharaoh?\"\n",
    "#q = \"Wasn't Menkauhor Kaiu one of the last Pharaohs of the Fifth Dynasty?\"\n",
    "#q = \"Shall Menkauhor Kaiu rule the Old Kingdom?\"\n",
    "#q = \"Was Djedefra the builder of the Sphinx?\"\n",
    "#q = \"Was Djoser the first Pharaoh of the old Kingdom?\"\n",
    "#qp = nlp(q)\n",
    "#print(len(qp))\n",
    "# displacy.render(qp, style=\"dep\", jupyter=True)\n",
    "# print([tok for tok in list(qp[3:]) if not tok.is_stop])\n",
    "answer_question(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_question_ents(nlp(\"Who was Kaiu?\"))\n",
    "#answer_question(\"Who was Kaiu?\")\n",
    "#list(nlp(\"Who was Kaiu?\").ents)[0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
